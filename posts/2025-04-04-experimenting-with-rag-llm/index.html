<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Experimenting with Retrieval Augmented Generation (RAG) with LLMs | DVLA Engineering</title><meta name=keywords content="Ruby,AI"><meta name=description content="Giving an LLM some external context with Ruby."><meta name=author content="Nigel Brookes-Thomas"><link rel=canonical href=https://dvla.github.io/posts/2025-04-04-experimenting-with-rag-llm/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://dvla.github.io/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dvla.github.io/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dvla.github.io/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://dvla.github.io/favicon/apple-touch-icon.png><link rel=mask-icon href=https://dvla.github.io/favicon/favicon-16x16.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-WH4N9FEEE8"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-WH4N9FEEE8",{anonymize_ip:!1})}</script><meta property="og:title" content="Experimenting with Retrieval Augmented Generation (RAG) with LLMs"><meta property="og:description" content="Giving an LLM some external context with Ruby."><meta property="og:type" content="article"><meta property="og:url" content="https://dvla.github.io/posts/2025-04-04-experimenting-with-rag-llm/"><meta property="og:image" content="https://dvla.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-04T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-04T00:00:00+00:00"><meta property="og:site_name" content="DVLA Engineering"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dvla.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Experimenting with Retrieval Augmented Generation (RAG) with LLMs"><meta name=twitter:description content="Giving an LLM some external context with Ruby."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dvla.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Experimenting with Retrieval Augmented Generation (RAG) with LLMs","item":"https://dvla.github.io/posts/2025-04-04-experimenting-with-rag-llm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Experimenting with Retrieval Augmented Generation (RAG) with LLMs","name":"Experimenting with Retrieval Augmented Generation (RAG) with LLMs","description":"Giving an LLM some external context with Ruby.","keywords":["Ruby, AI"],"articleBody":"Note well Please ensure you consider and adhere to any policies and restrictions your organisation places on the use of data with AI and the selection of AI models.\nI want to be able to ask an generative AI some questions while giving it the context from which I’d like it to use it’s smarts to derive an answer. This is Retrieval-Augmented Generation (RAG).\nBeing a Ruby engineer, I’m going to pick up my shiny red hammer to attack this problem.\nRunning a local LLM I’m going to run my LLM locally. I’m on a Mac, so I’m wanting the llama.cpp library installed. I’m going to need this for some dependencies later.\nbrew install llama_cpp Because I’m super lazy and want to experiment by hand, I’m using the open source Jan to host my model and, really conveniently, it can serve an Open AI compatible API. With Jan, I can easily pick and choose from a variety of different models or load my own.\nChatting to the LLM At the begining of this experiment, I wasn’t sure which model I wanted to use or how to interface with it so I used the langchainrb library which provides a high-level, pluggable interface.\nI do need to install the Open AI as well.\nThen I’m going to configure the Open AI library to use my local server rather than the internet. When I create a LLM client, I need to tell it which model I’m going to use. Since Jan can only load one model, I’m going to use the same one for all interations.\nrequire 'langchain' require 'openai' # logs are a bit chatty by default Langchain.logger.level = Logger::ERROR MODEL = 'llama3.2-3b-instruct' OpenAI.configure do |c| c.uri_base = 'http://127.0.0.1:1337/v1' end llm = Langchain::LLM::OpenAI.new(api_key: 'locally-model-no-api-key', default_options:{ chat_model: MODEL, completion_model: MODEL, embedding_model: MODEL } ) I also want an assistant client. An assistant stores context to make conversational interations more natural. I’m going to pass a block into the constructor which will be called as the response is streamed rather than wait until a complete result is received because I just want to print the response to the console as it is generated.\nassistant = Langchain::Assistant.new( llm:, instructions: \u003c\u003c~EO_PROMPT You are a very skilled and helpful assistant on the HR rules at the DVLA in the UK. You are able to find answers to the questions from the contextual passage snippets provided. Provide as much detail as you can. EO_PROMPT ) do |response_chunk| print response_chunk.dig('delta', 'content') end Collecting my own data I need to turn my unstructured source information into something a machine can deal with. For this experiment, I actually used some of our HR policies: they’re wordy, somewhat complex and the documents can be easily converted from Word to Markdown. I’m going to use Markdown section headers to identify coherent sections of text, something which works for these documents.\nI need a vector database to store this in. I’ve picked Milvus, mostly because it has a trivial quickstart through docker and a useful API wrapper in the Milvus gem.\nI can take my markdown sections, ask the LLM to generate embeddings, and push these into the vector DB. I also need to create a schema in Milvus, and the values here are almost certainly suboptimal.\ndb = Milvus::Client.new( url: 'http://localhost:19530' ) # in reality, ask Milvus if the collection exists first # before creating it db.collections.create( collection_name: 'hr', auto_id: true, fields: [ { fieldName: \"id\", isPrimary: true, autoID: false, dataType: \"Int64\" }, { fieldName: \"text\", dataType: \"VarChar\", elementTypeParams: { max_length: \"10000\" } }, { fieldName: \"vector\", dataType: \"FloatVector\", elementTypeParams: { dim: 3072 } } ] ) paras = File.read('hr-rules.md').split('# ').reject(\u0026:empty?) data = paras.map.with_index do |para, i| puts \"Adding document #{i}\" embeddings = llm.embed(text: para).embedding {vector: embeddings, text: para} # returning a hash end db.entities.insert(collection_name: 'hr', data:) Chatting Now I want to chat to this thing. When a person asks a question, I’m going to search the vector DB to locate any context I can find. I’m going to collect these results and pass this with the user query. The assistant client will remember the thread of conversation from one interaction to the next.\n# Ask Milvus to load the collection db.collections.load(collection_name: 'hr') embeddings = [] puts 'Ready to answer questions. Type \"exit\" to quit.' loop do query = gets if query == \"exit\\n\" break end embeddings \u003c\u003c llm.embed(text: query, model: 'llama3.2-3b-instruct').embedding context = db.entities.hybrid_search( collection_name: 'hr', search: embeddings.map { { anns_field: 'vector', data: [it], # Ruby v3.4 `it` block keyword output_fields: ['text'], limit: 5 } }, rerank: { strategy: 'rrf', params: { k: 10 } }, limit: 5, output_fields: ['text'])['data'].map { it['text'] }.join(\"\\n\\n\") prompt = \u003c\u003c~PROMPT Use the following pieces of information enclosed in \u003ccontext\u003e tags and from previous contexts to provide an answer to the question enclosed in \u003cquestion\u003e tags. Do not mention the \u003ccontext\u003e tags in your answer. \u003ccontext\u003e #{context} \u003c/context\u003e #{query} \u003c/question\u003e PROMPT assistant.add_message_and_run!(content: prompt) end Now I can happily chat with the AI about the information I’ve stored and ask it questions about it.\n","wordCount":"837","inLanguage":"en","datePublished":"2025-04-04T00:00:00Z","dateModified":"2025-04-04T00:00:00Z","author":{"@type":"Person","name":"Nigel Brookes-Thomas"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dvla.github.io/posts/2025-04-04-experimenting-with-rag-llm/"},"publisher":{"@type":"Organization","name":"DVLA Engineering","logo":{"@type":"ImageObject","url":"https://dvla.github.io/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dvla.github.io/ accesskey=h title="DVLA Engineering (Alt + H)"><img src=https://dvla.github.io/favicon/apple-touch-icon.png alt aria-label=logo height=49>DVLA Engineering</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dvla.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://dvla.github.io/open-source/ title=Open-source><span>Open-source</span></a></li><li><a href=https://github.com/dvla/ title=Github><span>Github</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://www.civil-service-careers.gov.uk/departments/working-for-the-driver-and-vehicle-licensing-agency/ title=Careers><span>Careers</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dvla.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://dvla.github.io/posts/>Posts</a></div><h1 class=post-title>Experimenting with Retrieval Augmented Generation (RAG) with LLMs</h1><div class=post-description>Giving an LLM some external context with Ruby.</div><div class=post-meta><span title='2025-04-04 00:00:00 +0000 UTC'>April 4, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;837 words&nbsp;·&nbsp;Nigel Brookes-Thomas</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#running-a-local-llm>Running a local LLM</a></li><li><a href=#chatting-to-the-llm>Chatting to the LLM</a></li><li><a href=#collecting-my-own-data>Collecting my own data</a></li><li><a href=#chatting>Chatting</a></li></ul></nav></div></details></div><div class=post-content><p><em>Note well</em> Please ensure you consider and adhere to any policies and restrictions your organisation places on the use of data with AI and the selection of AI models.</p><p>I want to be able to ask an generative AI some questions while giving it the context from which I&rsquo;d like it to use it&rsquo;s smarts to derive an answer. This is Retrieval-Augmented Generation (RAG).</p><p>Being a Ruby engineer, I&rsquo;m going to pick up my shiny red hammer to attack this problem.</p><h2 id=running-a-local-llm>Running a local LLM<a hidden class=anchor aria-hidden=true href=#running-a-local-llm>#</a></h2><p>I&rsquo;m going to run my LLM locally. I&rsquo;m on a Mac, so I&rsquo;m wanting the <code>llama.cpp</code> library installed. I&rsquo;m going to need this for some dependencies later.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>brew install llama_cpp
</span></span></code></pre></div><p>Because I&rsquo;m super lazy and want to experiment by hand, I&rsquo;m using the open source <a href=https://jan.ai>Jan</a> to host my model and, really conveniently, it can serve an Open AI compatible API. With Jan, I can easily pick and choose from a variety of different models or load my own.</p><h2 id=chatting-to-the-llm>Chatting to the LLM<a hidden class=anchor aria-hidden=true href=#chatting-to-the-llm>#</a></h2><p>At the begining of this experiment, I wasn&rsquo;t sure which model I wanted to use or how to interface with it so I used the <a href=https://github.com/patterns-ai-core/langchainrb><code>langchainrb</code></a> library which provides a high-level, pluggable interface.</p><p>I do need to install the <a href=https://rubygems.org/gems/ruby-openai>Open AI</a> as well.</p><p>Then I&rsquo;m going to configure the Open AI library to use my local server rather than the internet. When I create a LLM client, I need to tell it which model I&rsquo;m going to use. Since Jan can only load one model, I&rsquo;m going to use the same one for all interations.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl><span class=nb>require</span> <span class=s1>&#39;langchain&#39;</span>
</span></span><span class=line><span class=cl><span class=nb>require</span> <span class=s1>&#39;openai&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># logs are a bit chatty by default</span>
</span></span><span class=line><span class=cl><span class=no>Langchain</span><span class=o>.</span><span class=n>logger</span><span class=o>.</span><span class=n>level</span> <span class=o>=</span> <span class=no>Logger</span><span class=o>::</span><span class=no>ERROR</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=no>MODEL</span> <span class=o>=</span> <span class=s1>&#39;llama3.2-3b-instruct&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=no>OpenAI</span><span class=o>.</span><span class=n>configure</span> <span class=k>do</span> <span class=o>|</span><span class=n>c</span><span class=o>|</span>
</span></span><span class=line><span class=cl>  <span class=n>c</span><span class=o>.</span><span class=n>uri_base</span> <span class=o>=</span> <span class=s1>&#39;http://127.0.0.1:1337/v1&#39;</span>
</span></span><span class=line><span class=cl><span class=k>end</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=no>Langchain</span><span class=o>::</span><span class=no>LLM</span><span class=o>::</span><span class=no>OpenAI</span><span class=o>.</span><span class=n>new</span><span class=p>(</span><span class=ss>api_key</span><span class=p>:</span> <span class=s1>&#39;locally-model-no-api-key&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                 <span class=ss>default_options</span><span class=p>:{</span>
</span></span><span class=line><span class=cl>                                   <span class=ss>chat_model</span><span class=p>:</span> <span class=no>MODEL</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                   <span class=ss>completion_model</span><span class=p>:</span> <span class=no>MODEL</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                   <span class=ss>embedding_model</span><span class=p>:</span> <span class=no>MODEL</span> <span class=p>}</span> <span class=p>)</span>
</span></span></code></pre></div><p>I also want an assistant client. An assistant stores context to make conversational interations more natural. I&rsquo;m going to pass a block into the constructor which will be called as the response is streamed rather than wait until a complete result is received because I just want to print the response to the console as it is generated.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-Ruby data-lang=Ruby><span class=line><span class=cl><span class=n>assistant</span> <span class=o>=</span> <span class=no>Langchain</span><span class=o>::</span><span class=no>Assistant</span><span class=o>.</span><span class=n>new</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=ss>llm</span><span class=p>:,</span>
</span></span><span class=line><span class=cl>  <span class=ss>instructions</span><span class=p>:</span> <span class=o>&lt;&lt;~</span><span class=no>EO_PROMPT</span>
</span></span><span class=line><span class=cl>    <span class=no>You</span> <span class=n>are</span> <span class=n>a</span> <span class=n>very</span> <span class=n>skilled</span> <span class=ow>and</span> <span class=n>helpful</span> <span class=n>assistant</span> <span class=n>on</span> <span class=n>the</span> <span class=no>HR</span> <span class=n>rules</span> <span class=n>at</span> <span class=n>the</span> <span class=no>DVLA</span> <span class=k>in</span> <span class=n>the</span> <span class=no>UK</span><span class=o>.</span> <span class=no>You</span> <span class=n>are</span> <span class=n>able</span> <span class=n>to</span> <span class=n>find</span> <span class=n>answers</span> <span class=n>to</span> <span class=n>the</span> <span class=n>questions</span> <span class=n>from</span> <span class=n>the</span> <span class=n>contextual</span> <span class=n>passage</span> <span class=n>snippets</span> <span class=n>provided</span><span class=o>.</span> <span class=no>Provide</span> <span class=n>as</span> <span class=n>much</span> <span class=n>detail</span> <span class=n>as</span> <span class=n>you</span> <span class=n>can</span><span class=o>.</span>
</span></span><span class=line><span class=cl>  <span class=no>EO_PROMPT</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span> <span class=k>do</span> <span class=o>|</span><span class=n>response_chunk</span><span class=o>|</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span> <span class=n>response_chunk</span><span class=o>.</span><span class=n>dig</span><span class=p>(</span><span class=s1>&#39;delta&#39;</span><span class=p>,</span> <span class=s1>&#39;content&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>end</span>
</span></span></code></pre></div><h2 id=collecting-my-own-data>Collecting my own data<a hidden class=anchor aria-hidden=true href=#collecting-my-own-data>#</a></h2><p>I need to turn my unstructured source information into something a machine can deal with. For this experiment, I actually used some of our HR policies: they&rsquo;re wordy, somewhat complex and the documents can be easily converted from Word to Markdown. I&rsquo;m going to use Markdown section headers to identify coherent sections of text, something which works for these documents.</p><p>I need a vector database to store this in. I&rsquo;ve picked <a href=https://milvus.io>Milvus</a>, mostly because it has a trivial <a href=https://milvus.io/docs/install_standalone-docker.md>quickstart</a> through docker and a useful API wrapper in the <a href=https://github.com/patterns-ai-core/milvus>Milvus gem</a>.</p><p>I can take my markdown sections, ask the LLM to generate embeddings, and push these into the vector DB. I also need to create a schema in Milvus, and the values here are almost certainly suboptimal.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-Ruby data-lang=Ruby><span class=line><span class=cl><span class=n>db</span> <span class=o>=</span> <span class=no>Milvus</span><span class=o>::</span><span class=no>Client</span><span class=o>.</span><span class=n>new</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=ss>url</span><span class=p>:</span> <span class=s1>&#39;http://localhost:19530&#39;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># in reality, ask Milvus if the collection exists first</span>
</span></span><span class=line><span class=cl><span class=c1># before creating it</span>
</span></span><span class=line><span class=cl><span class=n>db</span><span class=o>.</span><span class=n>collections</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=ss>collection_name</span><span class=p>:</span> <span class=s1>&#39;hr&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=ss>auto_id</span><span class=p>:</span> <span class=kp>true</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=ss>fields</span><span class=p>:</span> <span class=o>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=ss>fieldName</span><span class=p>:</span> <span class=s2>&#34;id&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=ss>isPrimary</span><span class=p>:</span> <span class=kp>true</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=ss>autoID</span><span class=p>:</span> <span class=kp>false</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=ss>dataType</span><span class=p>:</span> <span class=s2>&#34;Int64&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=ss>fieldName</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=ss>dataType</span><span class=p>:</span> <span class=s2>&#34;VarChar&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=ss>elementTypeParams</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=ss>max_length</span><span class=p>:</span> <span class=s2>&#34;10000&#34;</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=ss>fieldName</span><span class=p>:</span> <span class=s2>&#34;vector&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=ss>dataType</span><span class=p>:</span> <span class=s2>&#34;FloatVector&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=ss>elementTypeParams</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=ss>dim</span><span class=p>:</span> <span class=mi>3072</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=o>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>paras</span> <span class=o>=</span> <span class=no>File</span><span class=o>.</span><span class=n>read</span><span class=p>(</span><span class=s1>&#39;hr-rules.md&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39;# &#39;</span><span class=p>)</span><span class=o>.</span><span class=n>reject</span><span class=p>(</span><span class=o>&amp;</span><span class=ss>:empty?</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>paras</span><span class=o>.</span><span class=n>map</span><span class=o>.</span><span class=n>with_index</span> <span class=k>do</span> <span class=o>|</span><span class=n>para</span><span class=p>,</span> <span class=n>i</span><span class=o>|</span>
</span></span><span class=line><span class=cl>  <span class=nb>puts</span> <span class=s2>&#34;Adding document </span><span class=si>#{</span><span class=n>i</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>embeddings</span> <span class=o>=</span> <span class=n>llm</span><span class=o>.</span><span class=n>embed</span><span class=p>(</span><span class=ss>text</span><span class=p>:</span> <span class=n>para</span><span class=p>)</span><span class=o>.</span><span class=n>embedding</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span><span class=ss>vector</span><span class=p>:</span> <span class=n>embeddings</span><span class=p>,</span> <span class=ss>text</span><span class=p>:</span> <span class=n>para</span><span class=p>}</span> <span class=c1># returning a hash</span>
</span></span><span class=line><span class=cl><span class=k>end</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>db</span><span class=o>.</span><span class=n>entities</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=ss>collection_name</span><span class=p>:</span> <span class=s1>&#39;hr&#39;</span><span class=p>,</span> <span class=ss>data</span><span class=p>:)</span>
</span></span></code></pre></div><h2 id=chatting>Chatting<a hidden class=anchor aria-hidden=true href=#chatting>#</a></h2><p>Now I want to chat to this thing. When a person asks a question, I&rsquo;m going to search the vector DB to locate any context I can find. I&rsquo;m going to collect these results and pass this with the user query. The assistant client will remember the thread of conversation from one interaction to the next.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl><span class=c1># Ask Milvus to load the collection</span>
</span></span><span class=line><span class=cl><span class=n>db</span><span class=o>.</span><span class=n>collections</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=ss>collection_name</span><span class=p>:</span> <span class=s1>&#39;hr&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>embeddings</span> <span class=o>=</span> <span class=o>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>puts</span> <span class=s1>&#39;Ready to answer questions. Type &#34;exit&#34; to quit.&#39;</span>
</span></span><span class=line><span class=cl><span class=kp>loop</span> <span class=k>do</span>
</span></span><span class=line><span class=cl>  <span class=n>query</span> <span class=o>=</span> <span class=nb>gets</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>query</span> <span class=o>==</span> <span class=s2>&#34;exit</span><span class=se>\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>break</span>
</span></span><span class=line><span class=cl>  <span class=k>end</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>embeddings</span> <span class=o>&lt;&lt;</span> <span class=n>llm</span><span class=o>.</span><span class=n>embed</span><span class=p>(</span><span class=ss>text</span><span class=p>:</span> <span class=n>query</span><span class=p>,</span> <span class=ss>model</span><span class=p>:</span> <span class=s1>&#39;llama3.2-3b-instruct&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>embedding</span>
</span></span><span class=line><span class=cl>  <span class=n>context</span> <span class=o>=</span> <span class=n>db</span><span class=o>.</span><span class=n>entities</span><span class=o>.</span><span class=n>hybrid_search</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                                      <span class=ss>collection_name</span><span class=p>:</span> <span class=s1>&#39;hr&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                      <span class=ss>search</span><span class=p>:</span> <span class=n>embeddings</span><span class=o>.</span><span class=n>map</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                                        <span class=p>{</span> <span class=ss>anns_field</span><span class=p>:</span> <span class=s1>&#39;vector&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                          <span class=ss>data</span><span class=p>:</span> <span class=o>[</span><span class=n>it</span><span class=o>]</span><span class=p>,</span> <span class=c1># Ruby v3.4 `it` block keyword</span>
</span></span><span class=line><span class=cl>                                          <span class=ss>output_fields</span><span class=p>:</span> <span class=o>[</span><span class=s1>&#39;text&#39;</span><span class=o>]</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                          <span class=ss>limit</span><span class=p>:</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl>                                        <span class=p>}</span>  <span class=p>},</span>
</span></span><span class=line><span class=cl>                                      <span class=ss>rerank</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                                        <span class=ss>strategy</span><span class=p>:</span> <span class=s1>&#39;rrf&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                        <span class=ss>params</span><span class=p>:</span> <span class=p>{</span> <span class=ss>k</span><span class=p>:</span> <span class=mi>10</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>                                      <span class=p>},</span>
</span></span><span class=line><span class=cl>                                      <span class=ss>limit</span><span class=p>:</span> <span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                      <span class=ss>output_fields</span><span class=p>:</span> <span class=o>[</span><span class=s1>&#39;text&#39;</span><span class=o>]</span><span class=p>)</span><span class=o>[</span><span class=s1>&#39;data&#39;</span><span class=o>].</span><span class=n>map</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                                        <span class=n>it</span><span class=o>[</span><span class=s1>&#39;text&#39;</span><span class=o>]</span>
</span></span><span class=line><span class=cl>                                      <span class=p>}</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>prompt</span> <span class=o>=</span> <span class=o>&lt;&lt;~</span><span class=no>PROMPT</span>
</span></span><span class=line><span class=cl>    <span class=no>Use</span> <span class=n>the</span> <span class=n>following</span> <span class=n>pieces</span> <span class=n>of</span> <span class=n>information</span> <span class=n>enclosed</span> <span class=k>in</span> <span class=o>&lt;</span><span class=n>context</span><span class=o>&gt;</span> <span class=n>tags</span> <span class=ow>and</span> <span class=n>from</span> <span class=n>previous</span> <span class=n>contexts</span> <span class=n>to</span> <span class=n>provide</span> <span class=n>an</span> <span class=n>answer</span> <span class=n>to</span> <span class=n>the</span> <span class=n>question</span> <span class=n>enclosed</span> <span class=k>in</span> <span class=o>&lt;</span><span class=n>question</span><span class=o>&gt;</span> <span class=n>tags</span><span class=o>.</span> <span class=no>Do</span> <span class=ow>not</span> <span class=n>mention</span> <span class=n>the</span> <span class=o>&lt;</span><span class=n>context</span><span class=o>&gt;</span> <span class=n>tags</span> <span class=k>in</span> <span class=n>your</span> <span class=n>answer</span><span class=o>.</span>
</span></span><span class=line><span class=cl>    <span class=o>&lt;</span><span class=n>context</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl>    <span class=c1>#{context}</span>
</span></span><span class=line><span class=cl>    <span class=o>&lt;</span><span class=sr>/context&gt;
</span></span></span><span class=line><span class=cl><span class=sr>    &lt;question&gt;
</span></span></span><span class=line><span class=cl><span class=sr>    </span><span class=si>#{</span><span class=n>query</span><span class=si>}</span><span class=sr>
</span></span></span><span class=line><span class=cl><span class=sr>    &lt;/</span><span class=n>question</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=no>PROMPT</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>assistant</span><span class=o>.</span><span class=n>add_message_and_run!</span><span class=p>(</span><span class=ss>content</span><span class=p>:</span> <span class=n>prompt</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>end</span>
</span></span></code></pre></div><p>Now I can happily chat with the AI about the information I&rsquo;ve stored and ask it questions about it.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://dvla.github.io/tags/ruby-ai/>Ruby, AI</a></li></ul><nav class=paginav><a class=next href=https://dvla.github.io/posts/2024-12-using-max-by-in-ruby/><span class=title>Next »</span><br><span>TiL: Using the max_by method in Ruby</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://dvla.github.io/>DVLA Engineering</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>