<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Improving our dead-letter queues | DVLA Engineering</title><meta name=keywords content="DLQ,AWS"><meta name=description content="Lessons learnt the hard way about dead letter retention limits and alarm configuration."><meta name=author content="Tom Collins"><link rel=canonical href=https://dvla.github.io/posts/2023-03-improving-our-dead-letter-queues/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://dvla.github.io/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dvla.github.io/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dvla.github.io/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://dvla.github.io/favicon/apple-touch-icon.png><link rel=mask-icon href=https://dvla.github.io/favicon/favicon-16x16.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-WH4N9FEEE8"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-WH4N9FEEE8",{anonymize_ip:!1})}</script><meta property="og:title" content="Improving our dead-letter queues"><meta property="og:description" content="Lessons learnt the hard way about dead letter retention limits and alarm configuration."><meta property="og:type" content="article"><meta property="og:url" content="https://dvla.github.io/posts/2023-03-improving-our-dead-letter-queues/"><meta property="og:image" content="https://dvla.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-10T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-10T00:00:00+00:00"><meta property="og:site_name" content="DVLA Engineering"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dvla.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Improving our dead-letter queues"><meta name=twitter:description content="Lessons learnt the hard way about dead letter retention limits and alarm configuration."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dvla.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Improving our dead-letter queues","item":"https://dvla.github.io/posts/2023-03-improving-our-dead-letter-queues/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Improving our dead-letter queues","name":"Improving our dead-letter queues","description":"Lessons learnt the hard way about dead letter retention limits and alarm configuration.","keywords":["DLQ","AWS"],"articleBody":"Introduction Dead letter queues (DLQ) are often used to catch items that have not been successfully processed or delivered when using queue 1 based integration patterns.\nTypically engineering teams will want to be alerted quickly to the presence of items on a dead letter queue so they can do something about it e.g. redrive messages back to a source queue, inspect the messages for further understanding of an issue etc.\nIn this post we‚Äôll discuss how we lost messages that had been placed on a dead-letter queue, how we have learnt from this experience and what we have done to improve our processes.\nBackground Using a source queue and a DLQ In a typical setup you may have:\nA source queue holding messages that need to be processed A handler that can consume messages from the queue and attempt to process them A dead-letter queue that can hold messages that can not be processed A means of alerting a human when messages are placed on the DLQ DLQ example using AWSThe message handler [2] invokes an external API. If the API is unavailable messages will end up on the DLQ, triggering an alarm.\nYou may not be able to process every message When handling a message various scenarios can result in not being unable to complete processing e.g.\nthe content of a message is invalid you have a defect in the handler a dependency, such as a HTTP API, is unavailable You may configure your implementation to retry the processing of a failed message multiple times, but at some point you will decide that it can not be handled and place it on a separate dead-letter queue [3].\nAt this point you probably want to alert a human [4] who can investigate and take appropriate action to resolve the situation.\nHandling items on a dead-letter queue We don‚Äôt want items appearing on our DLQ as it means something has gone wrong and remediating action may need to be taken. This could include:\nMove items back to the source queue If your messages were moved to the DLQ because a dependency was temporarily unavailable then you may just need to move the messages back onto the source queue (once the dependency is available) so they can be processed again.\nThe DLQ redrive feature as it appears in the AWS console. If you‚Äôre using AWS, and have the appropriate permissions, source queue re-drive can be achieved through the AWS console. This capability is not exposed through an API so may be something you need to implement yourself if the console is not an option in a production environment.\nInspecting messages to understand what has gone wrong You could also use a utility like the DVLA aws-sqs-utility to read message from the DLQ and inspect them to further understand an issue or defect. In some scenarios it may be appropriate to edit a message before moving it back onto the source queue.\nThink about your DLQ processes when designing your solution Other scenarios and queuing services may require a different approach and you should think about, and test, this, and the permissions required, while designing your solution, especially if the processing of items is time sensitive. The permissions required could include read and write permissions for multiple queues and being able to decrypt and encrypt messages.\nAWS SQS and maximum message retention SQS has a maximum message retention period of 14 days We use AWS SQS for both source queues and a dead-letter queues and SQS has a maximum message retention period of fourteen days. Once a message has sat on a queue for this duration the message will be deleted.\nIn general fourteen days is more then adequate for message processing, even accounting for retries and backoff.\nMessages can sit on a DLQ waiting for action to be taken But what happens when your message sits on a DLQ waiting for a human to take action?\nIf a human is alerted quickly and takes appropriate action within fourteen days then everything is fine If a human is not alerted quickly, or does not take appropriate action, then the message is going to sit on the queue until it is deleted and lost forever How we lost some messages You can probably work this out by now but:\nüëé 20+ critical messages landed on a SQS DLQ üëç a CloudWatch alarm was triggered üëç a human was alerted quickly via a PagerDuty alert üëç a human looked at the incident üëé but the human did not take appropriate action ‚åõ the messages sat on the queue for 14 days üíÄ the messages hit the maximum retention limit and were deleted ‚åõ we didn‚Äôt discover this until several weeks later It‚Äôs not about blaming the human Humans make mistakes, defects will make it to production, this is ok, its going to happen, its not about blame and this is how we learn and gain experience.\nWhat is important is how you respond and how you identify and implement actions to avoid repeating the same mistake again.\nWhat happened next This time we were lucky, we were able to perform reconciliation activities and recreate the lost messages.\nThis involved verifying that each source record from this period had been processed by the target system. For any records that had not been processed we were able to recreate the lost message, using the data in the source record, and complete processing.\nIn another context this could have been far worse e.g.\nthere may not have been a source record to aid with reconciliation, meaning the messages really were lost forever the volume of messages being handled, or period of data loss, could have been significantly greater requiring a far more significant effort to investigate and resolve How we have improved our DLQ alerting We don‚Äôt want to lose data, it is critical we are notified when messages land on a DLQ\nClearly we don‚Äôt want to lose messages, so we spent some time analysing what went wrong and what we could do to improve our alarms and alerting.\nUnderstanding our DLQ alarm behaviour Our original alarm threshold was triggered once when messages were initially added to the queue.\nMessages were added to the DLQ in distinct phases over a period of time but just a single alarm was triggered and a single incident raised.\nThis isn‚Äôt ideal so we asked ourselves how we could trigger multiple alarms to increase awarness of the issue and the likelihood of it being handled correctly.\nHow was our alarm configured? The alarm threshold logic was:\nif the number of messages on the DLQ is \u003e 0 then raise an alarm\nWhich turned out to be a little naive causing just a single alarm to be triggered when the DLQ first moved to \u003e 0.\nWe would not get any further alarms being triggered while the number of items on the queue remained above zero, even if messages were being added to the DLQ every 10 minutes for the next 4 weeks.\nTime Error DLQ Size DLQ Size \u003e 0 Alarm State Alert 07:50 - 0 - OK 07:53 ‚úîÔ∏è 3 ‚úîÔ∏è ALARM üîî 08:00 - 3 ‚úîÔ∏è ALARM 08:02 ‚úîÔ∏è 10 ‚úîÔ∏è ALARM 08:10 - 10 ‚úîÔ∏è ALARM 08:13 ‚úîÔ∏è 11 ‚úîÔ∏è ALARM The alarm threshold was met once at 07:53 and entered an ALARM state, but never returned to an OK state so was never in a position to be triggered again.\nFiring an alarm each time messages are added to the DLQ We discussed the incident at one of our regular engineering community of practice meetings, both to raise awareness and to try and learn from each other. One of our teams shared their approach:\nif the number of messages added to the DLQ in the last 5 minutes \u003e 0 then raise an alarm\nWhich results in the alarm being triggered multiple times:\nTime Error DLQ Size DLQ Size (5 mins) Alarm State Alert 07:50 - 0 0 OK 07:53 ‚úîÔ∏è 3 3 ALARM üîî 08:00 - 3 0 OK 08:02 ‚úîÔ∏è 10 7 ALARM üîî 08:10 - 10 0 OK 08:13 ‚úîÔ∏è 11 1 ALARM üîî The threshold is met at 07:53 entering an ALARM state then returns to OK within five minutes. This allows the threshold to crossed again when subsequent messages are added to the queue (08:02, 08:13). Much nicer.\nNow the alarm triggers multiple times.\nIf you want to set his up in Cloudwatch it will look something like this:\nDLQueueAlarm: Type: AWS::CloudWatch::Alarm Properties: AlarmDescription: 'Message(s) added to the SMS DLQ in the last 5 minutes' Namespace: 'AWS/SQS' MetricName: NumberOfMessagesSent Dimensions: - Name: QueueName Value: 'my-dlq' Statistic: Sum Period: 60 EvaluationPeriods: 5 DatapointsToAlarm: 1 Threshold: 1 ComparisonOperator: GreaterThanOrEqualToThreshold AlarmActions: - Ref: DLQueueAlarmTopic Update 2022-04-04 - it turns out that this example only works when manually adding messages to your DLQ (which you may do while testing thge alarm). You can achieve something equivilent using RATE(ApproximateNumberOfMessagesVisible)\u003e0 and I‚Äôll update this example to reflect this ASAP.\nThanks to Sam Dengler for getting in touch to point this out. For more info see NumberOfMessagesSent does not increase as the result of a failure in the source queue\nFiring an alarm every day if there are messages on the DLQ We also thought about what we could do to minimise the impact of a human not handling an incident correctly and decided that it would be helpful to raise a new alert for each day that we had messages sat on a DLQ.\nOne of our developers came up with a really neat cloudwatch expression that makes this possible through simple config :\nIF(HOUR(myDLQ)\u003e7 AND HOUR(myDLQ)\u003c9 AND myDLQ \u003e 0, 1, 0)\nWhich will result in the alarm being triggered at around 7am each day if there are messages on the DLQ, even if no further messages have been added.\nDaily DLQ alarmThe expression means that the alarm enters an ALARM state at 7 and returns to OK at 9.\nKudos to Matthew Lewis for figuring this out üôå\nAnd in CloudFormation:\nDLQDailyAlarm: Type: AWS::CloudWatch::Alarm Properties: ActionsEnabled: True AlarmActions: - arn:aws:sns:eu-west-2:1234567890:alert-topic AlarmDescription: Daily alarm for messages on DLQ AlarmName: 'dlq-daily-alarm' Metrics: - Id: summary Label: DLD Dead Letter Queues Alarm Expression: IF(HOUR(myDLQ)\u003e7 AND HOUR(myDLQ)\u003c9 AND myDLQ \u003e 0, 1, 0) ReturnData: true Looking at our PagerDuty integration We use PagerDuty to manage incidents and alert engineers.\nWhen integrating Cloudwatch alarms with PagerDuty to raise Alerts there are a few nuances to be aware of with this sort of alarm threshold.\nOk actions and auto-resolving alerts\nWhen the number of messages added to the DQL within 5 minutes drops to zero the cloudwatch alarm will return to an OK state\nIf you integrate a Cloudwatch OK action with PagerDuty this will cause the PagerDuty Alert to be auto-resolved and closed before a human has had time to notice and take appropriate action.\nConfigure alert correlation\nTo ensure that you raise a new alert each day you will need to configure how PagerDuty performs correlation. If you don‚Äôt do this you may find that your daily alarms are grouped together under the initial alert.\nCorrelate events by: Make a new incident/alert each time\nDerive name from: Alarm Name\nConfigure your Cloudwatch integration within PagerDuty to ensure a new incident is raised each time your alarm triggers. Other Improvements Persisting a copy of our messages We are also reviewing out patterns to ensure we can always persist a copy of a messages, or the data required to create a message, for a reasonable duration of time, which would help is scenarios like this.\nThis could include:\nensuring we always write the data or a message into a database or other persistant store before writing to a queue using Amazon EventBridge instead of SQS as it has an archive and replay capability. adding Amazon SNS in front of SQS and using the fan-out pattern to archive messages in S3. Summary Hopefully this article will help you think a little more about how you are using DLQs and the processes you have in place to handle any messages that land on them.\nAll being well this isn‚Äôt someything you‚Äôll have to do frequently but it may be worth reviewing your approach and what alarms or alerts you have in place.\nTo be more accurate DLQs are used with a wide within a range of service integration patterns (queues, buses, topics etc) and can be plugged into many cloud vendor managed services to help with processing or delivery failures.¬†‚Ü©Ô∏é\n","wordCount":"2086","inLanguage":"en","datePublished":"2023-03-10T00:00:00Z","dateModified":"2023-03-10T00:00:00Z","author":{"@type":"Person","name":"Tom Collins"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dvla.github.io/posts/2023-03-improving-our-dead-letter-queues/"},"publisher":{"@type":"Organization","name":"DVLA Engineering","logo":{"@type":"ImageObject","url":"https://dvla.github.io/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dvla.github.io/ accesskey=h title="DVLA Engineering (Alt + H)"><img src=https://dvla.github.io/favicon/apple-touch-icon.png alt aria-label=logo height=49>DVLA Engineering</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dvla.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://dvla.github.io/open-source/ title=Open-source><span>Open-source</span></a></li><li><a href=https://github.com/dvla/ title=Github><span>Github</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://www.civil-service-careers.gov.uk/departments/working-for-the-driver-and-vehicle-licensing-agency/ title=Careers><span>Careers</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dvla.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://dvla.github.io/posts/>Posts</a></div><h1 class=post-title>Improving our dead-letter queues</h1><div class=post-description>Lessons learnt the hard way about dead letter retention limits and alarm configuration.</div><div class=post-meta><span title='2023-03-10 00:00:00 +0000 UTC'>March 10, 2023</span>&nbsp;¬∑&nbsp;10 min&nbsp;¬∑&nbsp;2086 words&nbsp;¬∑&nbsp;Tom Collins</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#background>Background</a><ul><li><a href=#using-a-source-queue-and-a-dlq>Using a source queue and a DLQ</a></li><li><a href=#you-may-not-be-able-to-process-every-message>You may not be able to process every message</a></li></ul></li><li><a href=#handling-items-on-a-dead-letter-queue>Handling items on a dead-letter queue</a><ul><li><a href=#move-items-back-to-the-source-queue>Move items back to the source queue</a></li><li><a href=#inspecting-messages-to-understand-what-has-gone-wrong>Inspecting messages to understand what has gone wrong</a></li><li><a href=#think-about-your-dlq-processes-when-designing-your-solution>Think about your DLQ processes when designing your solution</a></li></ul></li><li><a href=#aws-sqs-and-maximum-message-retention>AWS SQS and maximum message retention</a><ul><li><a href=#sqs-has-a-maximum-message-retention-period-of-14-days>SQS has a maximum message retention period of 14 days</a></li><li><a href=#messages-can-sit-on-a-dlq-waiting-for-action-to-be-taken>Messages can sit on a DLQ waiting for action to be taken</a></li></ul></li><li><a href=#how-we-lost-some-messages>How we lost some messages</a><ul><li><a href=#its-not-about-blaming-the-human>It&rsquo;s not about blaming the human</a></li><li><a href=#what-happened-next>What happened next</a></li></ul></li><li><a href=#how-we-have-improved-our-dlq-alerting>How we have improved our DLQ alerting</a><ul><li><a href=#understanding-our-dlq-alarm-behaviour>Understanding our DLQ alarm behaviour</a></li><li><a href=#firing-an-alarm-each-time-messages-are-added-to-the-dlq>Firing an alarm each time messages are added to the DLQ</a></li><li><a href=#firing-an-alarm-every-day-if-there-are-messages-on-the-dlq>Firing an alarm every day if there are messages on the DLQ</a></li><li><a href=#looking-at-our-pagerduty-integration>Looking at our PagerDuty integration</a></li></ul></li><li><a href=#other-improvements>Other Improvements</a><ul><li><a href=#persisting-a-copy-of-our-messages>Persisting a copy of our messages</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Dead letter queues (DLQ) are often used to catch items that have not been successfully processed or delivered when using queue <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> based integration patterns.</p><p>Typically engineering teams will want to be alerted quickly to the presence of items on a dead letter queue so they can do something about it e.g. <a href=https://aws.amazon.com/blogs/compute/introducing-amazon-simple-queue-service-dead-letter-queue-redrive-to-source-queues/>redrive messages back to a source queue</a>, inspect the messages for further understanding of an issue etc.</p><p>In this post we&rsquo;ll discuss how we lost messages that had been placed on a dead-letter queue, how we have learnt from this experience and what we have done to improve our processes.</p><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><h3 id=using-a-source-queue-and-a-dlq>Using a source queue and a DLQ<a hidden class=anchor aria-hidden=true href=#using-a-source-queue-and-a-dlq>#</a></h3><p>In a typical setup you may have:</p><ol><li>A source queue holding messages that need to be processed</li><li>A handler that can consume messages from the queue and attempt to process them</li><li>A dead-letter queue that can hold messages that can not be processed</li><li>A means of alerting a human when messages are placed on the DLQ</li></ol><figure><img loading=lazy src=images/queue-with-dlq-and-alarm.png alt="The message handler [2] invokes an external API. If the API is unavailable messages will end up on the DLQ, triggering an alarm."><figcaption>DLQ example using AWS<p>The message handler [2] invokes an external API. If the API is unavailable messages will end up on the DLQ, triggering an alarm.</p></figcaption></figure><h3 id=you-may-not-be-able-to-process-every-message>You may not be able to process every message<a hidden class=anchor aria-hidden=true href=#you-may-not-be-able-to-process-every-message>#</a></h3><p>When handling a message various scenarios can result in not being unable to complete processing e.g.</p><ul><li>the content of a message is invalid</li><li>you have a defect in the handler</li><li>a dependency, such as a HTTP API, is unavailable</li></ul><p>You may configure your implementation to retry the processing of a failed message multiple times, but at some point you will decide that it can not be handled and place it on a separate dead-letter queue [3].</p><p><em>At this point you probably want to alert a human [4] who can investigate and take appropriate action to resolve the situation.</em></p><hr><h2 id=handling-items-on-a-dead-letter-queue>Handling items on a dead-letter queue<a hidden class=anchor aria-hidden=true href=#handling-items-on-a-dead-letter-queue>#</a></h2><p>We don&rsquo;t want items appearing on our DLQ as it means something has gone wrong and remediating action may need to be taken. This could include:</p><h3 id=move-items-back-to-the-source-queue>Move items back to the source queue<a hidden class=anchor aria-hidden=true href=#move-items-back-to-the-source-queue>#</a></h3><p>If your messages were moved to the DLQ because a dependency was temporarily unavailable then you may just need to move the messages back onto the source queue (once the dependency is available) so they can be processed again.</p><figure><img loading=lazy src=images/aws-sqs-dlq-redrive.jpeg><figcaption>The DLQ redrive feature as it appears in the AWS console.</figcaption></figure><p>If you&rsquo;re using AWS, and have the appropriate permissions, source queue re-drive can be achieved <a href=https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-dead-letter-queue-redrive.html>through the AWS console</a>. This capability is not exposed through an API so may be something you need to implement yourself if the console is not an option in a production environment.</p><h3 id=inspecting-messages-to-understand-what-has-gone-wrong>Inspecting messages to understand what has gone wrong<a hidden class=anchor aria-hidden=true href=#inspecting-messages-to-understand-what-has-gone-wrong>#</a></h3><p>You could also use a utility like the DVLA <a href=https://github.com/dvla/aws-sqs-utility>aws-sqs-utility</a> to read message from the DLQ and inspect them to further understand an issue or defect. In <em>some</em> scenarios it may be appropriate to edit a message before moving it back onto the source queue.</p><h3 id=think-about-your-dlq-processes-when-designing-your-solution>Think about your DLQ processes when designing your solution<a hidden class=anchor aria-hidden=true href=#think-about-your-dlq-processes-when-designing-your-solution>#</a></h3><p>Other scenarios and queuing services may require a different approach and you should think about, and test, this, and the permissions required, while designing your solution, especially if the processing of items is time sensitive. The permissions required could include read and write permissions for multiple queues and being able to decrypt and encrypt messages.</p><hr><h2 id=aws-sqs-and-maximum-message-retention>AWS SQS and maximum message retention<a hidden class=anchor aria-hidden=true href=#aws-sqs-and-maximum-message-retention>#</a></h2><h3 id=sqs-has-a-maximum-message-retention-period-of-14-days>SQS has a maximum message retention period of 14 days<a hidden class=anchor aria-hidden=true href=#sqs-has-a-maximum-message-retention-period-of-14-days>#</a></h3><figure><img loading=lazy src=images/sqs-deletes-messages.png></figure><p>We use AWS SQS for both source queues and a dead-letter queues and SQS has a <a href=https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html>maximum message retention period</a> of fourteen days. Once a message has sat on a queue for this duration <em>the message will be deleted</em>.</p><p>In general fourteen days is more then adequate for message processing, even accounting for retries and backoff.</p><h3 id=messages-can-sit-on-a-dlq-waiting-for-action-to-be-taken>Messages can sit on a DLQ waiting for action to be taken<a hidden class=anchor aria-hidden=true href=#messages-can-sit-on-a-dlq-waiting-for-action-to-be-taken>#</a></h3><p>But what happens when your message sits on a DLQ waiting for a human to take action?</p><ul><li>If a human is alerted quickly and takes appropriate action within fourteen days then everything is fine</li><li>If a human is not alerted quickly, or does not take appropriate action, then the message is going to sit on the queue until it is <strong>deleted and lost forever</strong></li></ul><hr><h2 id=how-we-lost-some-messages>How we lost some messages<a hidden class=anchor aria-hidden=true href=#how-we-lost-some-messages>#</a></h2><p>You can probably work this out by now but:</p><ul><li>üëé 20+ critical messages landed on a SQS DLQ</li><li>üëç a CloudWatch alarm was triggered</li><li>üëç a human was alerted quickly via a PagerDuty alert</li><li>üëç a human looked at the incident</li><li>üëé but the human did not take appropriate action</li><li>‚åõ the messages sat on the queue for 14 days</li><li>üíÄ the messages hit the maximum retention limit and were deleted</li><li>‚åõ we didn&rsquo;t discover this until several weeks later</li></ul><h3 id=its-not-about-blaming-the-human>It&rsquo;s not about blaming the human<a hidden class=anchor aria-hidden=true href=#its-not-about-blaming-the-human>#</a></h3><p>Humans make mistakes, defects will make it to production, this is ok, its going to happen, its not about blame and this is how we learn and gain experience.</p><p>What is important is how you respond and how you identify and implement actions to avoid repeating the same mistake again.</p><h3 id=what-happened-next>What happened next<a hidden class=anchor aria-hidden=true href=#what-happened-next>#</a></h3><p>This time we were lucky, we were able to perform reconciliation activities and recreate the lost messages.</p><p>This involved verifying that each source record from this period had been processed by the target system. For any records that had not been processed we were able to recreate the lost message, using the data in the source record, and complete processing.</p><p>In another context this could have been far worse e.g.</p><ul><li>there may not have been a source record to aid with reconciliation, meaning the messages really were lost forever</li><li>the volume of messages being handled, or period of data loss, could have been significantly greater requiring a far more significant effort to investigate and resolve</li></ul><hr><h2 id=how-we-have-improved-our-dlq-alerting>How we have improved our DLQ alerting<a hidden class=anchor aria-hidden=true href=#how-we-have-improved-our-dlq-alerting>#</a></h2><blockquote><p>We don&rsquo;t want to lose data, it is critical we are notified when messages land on a DLQ</p></blockquote><p>Clearly we don&rsquo;t want to lose messages, so we spent some time analysing what went wrong and what we could do to improve our alarms and alerting.</p><h3 id=understanding-our-dlq-alarm-behaviour>Understanding our DLQ alarm behaviour<a hidden class=anchor aria-hidden=true href=#understanding-our-dlq-alarm-behaviour>#</a></h3><figure><img loading=lazy src=images/approx-number-of-messages-visible-with-alarm.png alt="Our original alarm threshold was triggered once when messages were initially  added to the queue."><figcaption><p>Our original alarm threshold was triggered once when messages were initially added to the queue.</p></figcaption></figure><p>Messages were added to the DLQ in distinct phases over a period of time but just a single alarm was triggered and a single incident raised.</p><p>This isn&rsquo;t ideal so we asked ourselves how we could trigger multiple alarms to increase awarness of the issue and the likelihood of it being handled correctly.</p><h4 id=how-was-our-alarm-configured>How was our alarm configured?<a hidden class=anchor aria-hidden=true href=#how-was-our-alarm-configured>#</a></h4><p>The alarm threshold logic was:</p><blockquote><p>if the number of messages on the DLQ is > 0 then raise an alarm</p></blockquote><p>Which turned out to be a little naive causing just a single alarm to be triggered when the DLQ first moved to > 0.</p><p>We would not get any further alarms being triggered while the number of items on the queue remained above zero, even if messages were being added to the DLQ every 10 minutes for the next 4 weeks.</p><table><thead><tr><th>Time</th><th>Error</th><th>DLQ Size</th><th>DLQ Size > 0</th><th>Alarm State</th><th>Alert</th></tr></thead><tbody><tr><td>07:50</td><td>-</td><td>0</td><td>-</td><td><code>OK</code></td><td></td></tr><tr><td>07:53</td><td>‚úîÔ∏è</td><td>3</td><td>‚úîÔ∏è</td><td><code>ALARM</code></td><td>üîî</td></tr><tr><td>08:00</td><td>-</td><td>3</td><td>‚úîÔ∏è</td><td><code>ALARM</code></td><td></td></tr><tr><td>08:02</td><td>‚úîÔ∏è</td><td>10</td><td>‚úîÔ∏è</td><td><code>ALARM</code></td><td></td></tr><tr><td>08:10</td><td>-</td><td>10</td><td>‚úîÔ∏è</td><td><code>ALARM</code></td><td></td></tr><tr><td>08:13</td><td>‚úîÔ∏è</td><td>11</td><td>‚úîÔ∏è</td><td><code>ALARM</code></td><td></td></tr></tbody></table><p>The alarm threshold was met once at 07:53 and entered an <code>ALARM</code> state, but never returned to an <code>OK</code> state so was never in a position to be triggered again.</p><h3 id=firing-an-alarm-each-time-messages-are-added-to-the-dlq>Firing an alarm each time messages are added to the DLQ<a hidden class=anchor aria-hidden=true href=#firing-an-alarm-each-time-messages-are-added-to-the-dlq>#</a></h3><p>We discussed the incident at one of our regular engineering community of practice meetings, both to raise awareness and to try and learn from each other. One of our teams shared their approach:</p><blockquote><p>if the number of messages added to the DLQ in the last 5 minutes > 0 then raise an alarm</p></blockquote><p>Which results in the alarm being triggered multiple times:</p><table><thead><tr><th>Time</th><th>Error</th><th>DLQ Size</th><th>DLQ Size (5 mins)</th><th>Alarm State</th><th>Alert</th></tr></thead><tbody><tr><td>07:50</td><td>-</td><td>0</td><td>0</td><td><code>OK</code></td><td></td></tr><tr><td>07:53</td><td>‚úîÔ∏è</td><td>3</td><td><strong>3</strong></td><td><code>ALARM</code></td><td>üîî</td></tr><tr><td>08:00</td><td>-</td><td>3</td><td>0</td><td><code>OK</code></td><td></td></tr><tr><td>08:02</td><td>‚úîÔ∏è</td><td>10</td><td><strong>7</strong></td><td><code>ALARM</code></td><td>üîî</td></tr><tr><td>08:10</td><td>-</td><td>10</td><td>0</td><td><code>OK</code></td><td></td></tr><tr><td>08:13</td><td>‚úîÔ∏è</td><td>11</td><td><strong>1</strong></td><td><code>ALARM</code></td><td>üîî</td></tr></tbody></table><p>The threshold is met at 07:53 entering an <code>ALARM</code> state then returns to <code>OK</code> within five minutes. This allows the threshold to crossed again when subsequent messages are added to the queue (08:02, 08:13). Much nicer.</p><figure><img loading=lazy src=images/approx-number-of-messages-visible-with-alarms.png alt="Now the alarm triggers multiple times."><figcaption><p>Now the alarm triggers multiple times.</p></figcaption></figure><p>If you want to set his up in Cloudwatch it will look something like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yml data-lang=yml><span class=line><span class=cl><span class=nt>DLQueueAlarm</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>Type</span><span class=p>:</span><span class=w> </span><span class=l>AWS::CloudWatch::Alarm</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>Properties</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>AlarmDescription</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;Message(s) added to the SMS DLQ in the last 5 minutes&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>Namespace</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;AWS/SQS&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>MetricName</span><span class=p>:</span><span class=w> </span><span class=l>NumberOfMessagesSent</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>Dimensions</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>Name</span><span class=p>:</span><span class=w> </span><span class=l>QueueName</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>Value</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;my-dlq&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>Statistic</span><span class=p>:</span><span class=w> </span><span class=l>Sum</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>Period</span><span class=p>:</span><span class=w> </span><span class=m>60</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>EvaluationPeriods</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>DatapointsToAlarm</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>Threshold</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>ComparisonOperator</span><span class=p>:</span><span class=w> </span><span class=l>GreaterThanOrEqualToThreshold</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>AlarmActions</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>Ref</span><span class=p>:</span><span class=w> </span><span class=l>DLQueueAlarmTopic</span><span class=w>
</span></span></span></code></pre></div><blockquote><p>Update 2022-04-04 - it turns out that this example only works when manually adding messages to your DLQ (which you may do while testing thge alarm). You can achieve something equivilent using <code>RATE(ApproximateNumberOfMessagesVisible)>0</code> and I&rsquo;ll update this example to reflect this ASAP.</p></blockquote><blockquote><p><em>Thanks to <a href=https://www.linkedin.com/in/samdengler/>Sam Dengler</a> for getting in touch to point this out. For more info see <a href=https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html#sqs-dead-letter-queues-troubleshooting>NumberOfMessagesSent does not increase as the result of a failure in the source queue</a></em></p></blockquote><h3 id=firing-an-alarm-every-day-if-there-are-messages-on-the-dlq>Firing an alarm every day if there are messages on the DLQ<a hidden class=anchor aria-hidden=true href=#firing-an-alarm-every-day-if-there-are-messages-on-the-dlq>#</a></h3><p>We also thought about what we could do to minimise the impact of a human not handling an incident correctly and decided that it would be helpful to raise a new alert for each day that we had messages sat on a DLQ.</p><p>One of our developers came up with a really neat cloudwatch expression that makes this possible through simple config :</p><blockquote><p>IF(HOUR(myDLQ)>7 AND HOUR(myDLQ)&lt;9 AND myDLQ > 0, 1, 0)</p></blockquote><p>Which will result in the alarm being triggered at around 7am each day if there are messages on the DLQ, even if no further messages have been added.</p><figure><img loading=lazy src=images/sqs-daily-alarm.png alt="The expression means that the alarm enters an ALARM state at 7 and returns to OK at 9."><figcaption>Daily DLQ alarm<p>The expression means that the alarm enters an ALARM state at 7 and returns to OK at 9.</p></figcaption></figure><p><em>Kudos to <a href=https://www.linkedin.com/in/matthew-lewis-277a7157/>Matthew Lewis</a> for figuring this out üôå</em></p><p>And in CloudFormation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yml data-lang=yml><span class=line><span class=cl><span class=nt>DLQDailyAlarm</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>Type</span><span class=p>:</span><span class=w> </span><span class=l>AWS::CloudWatch::Alarm</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>Properties</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>ActionsEnabled</span><span class=p>:</span><span class=w> </span><span class=kc>True</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>AlarmActions</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>arn:aws:sns:eu-west-2:1234567890:alert-topic</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>AlarmDescription</span><span class=p>:</span><span class=w> </span><span class=l>Daily alarm for messages on DLQ</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>AlarmName</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;dlq-daily-alarm&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>Metrics</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>Id</span><span class=p>:</span><span class=w> </span><span class=l>summary</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>Label</span><span class=p>:</span><span class=w> </span><span class=l>DLD Dead Letter Queues Alarm</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>Expression</span><span class=p>:</span><span class=w> </span><span class=l>IF(HOUR(myDLQ)&gt;7 AND HOUR(myDLQ)&lt;9 AND myDLQ &gt; 0, 1, 0)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>ReturnData</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span></code></pre></div><h3 id=looking-at-our-pagerduty-integration>Looking at our PagerDuty integration<a hidden class=anchor aria-hidden=true href=#looking-at-our-pagerduty-integration>#</a></h3><p>We use PagerDuty to manage incidents and alert engineers.</p><p>When integrating Cloudwatch alarms with PagerDuty to raise <a href=https://support.pagerduty.com/docs/alerts>Alerts</a> there are a few nuances to be aware of with this sort of alarm threshold.</p><p><strong>Ok actions and auto-resolving alerts</strong></p><p>When the number of messages added to the DQL within 5 minutes drops to zero the cloudwatch alarm will return to an <code>OK</code> state</p><p>If you integrate a Cloudwatch <code>OK</code> action with PagerDuty this will cause the PagerDuty Alert to be auto-resolved and closed before a human has had time to notice and take appropriate action.</p><p><strong>Configure alert correlation</strong></p><p>To ensure that you raise a new alert each day you will need to configure how PagerDuty performs correlation. If you don&rsquo;t do this you may find that your daily alarms are grouped together under the initial alert.</p><p><strong>Correlate events by:</strong> Make a new incident/alert each time</p><p><strong>Derive name from:</strong> Alarm Name</p><figure><img loading=lazy src=images/cloudwatch-pagerduty-integration.png><figcaption>Configure your Cloudwatch integration within PagerDuty to ensure a new incident is raised each time your alarm triggers.</figcaption></figure><hr><h2 id=other-improvements>Other Improvements<a hidden class=anchor aria-hidden=true href=#other-improvements>#</a></h2><h3 id=persisting-a-copy-of-our-messages>Persisting a copy of our messages<a hidden class=anchor aria-hidden=true href=#persisting-a-copy-of-our-messages>#</a></h3><p>We are also reviewing out patterns to ensure we can always persist a copy of a messages, or the data required to create a message, for a reasonable duration of time, which would help is scenarios like this.</p><p>This could include:</p><ul><li>ensuring we always write the data or a message into a database or other persistant store before writing to a queue</li><li>using Amazon EventBridge instead of SQS as it has an <a href=https://aws.amazon.com/blogs/aws/new-archive-and-replay-events-with-amazon-eventbridge/>archive and replay</a> capability.</li><li>adding Amazon SNS in front of SQS and <a href=https://docs.aws.amazon.com/sns/latest/dg/firehose-example-use-case.html>using the fan-out pattern to archive messages in S3</a>.</li></ul><hr><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>Hopefully this article will help you think a little more about how you are using DLQs and the processes you have in place to handle any messages that land on them.</p><p>All being well this isn&rsquo;t someything you&rsquo;ll have to do frequently but it may be worth reviewing your approach and what alarms or alerts you have in place.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>To be more accurate DLQs are used with a wide within a range of service integration patterns (queues, buses, topics etc) and can be plugged into many cloud vendor managed services to help with processing or delivery failures.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://dvla.github.io/tags/dlq/>DLQ</a></li><li><a href=https://dvla.github.io/tags/aws/>AWS</a></li></ul><nav class=paginav><a class=next href=https://dvla.github.io/posts/2022-10-working-with-json-schema/><span class=title>Next ¬ª</span><br><span>Working with JSON Schema</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://dvla.github.io/>DVLA Engineering</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>